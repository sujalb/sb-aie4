{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "âœ‹BREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LCEL RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "1f43fdba-f1b4-4cf4-b8b0-3469edc29c80"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere lxml -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:.env file found at: c:\\src\\mlops\\sb-aie4\\.env\n",
            "INFO:__main__:OpenAI API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = Path.cwd()\n",
        "\n",
        "# Construct the path to the .env file\n",
        "env_path = current_dir.parent.parent / '.env'\n",
        "\n",
        "# Check if the .env file exists\n",
        "if env_path.exists():\n",
        "    logger.info(f\".env file found at: {env_path}\")\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv(dotenv_path=env_path)\n",
        "else:\n",
        "    logger.error(f\".env file not found at: {env_path}\")\n",
        "\n",
        "# Retrieve the OpenAI API key from environment variables\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Verify that the API key is set\n",
        "if openai.api_key:\n",
        "    logger.info(\"OpenAI API key loaded successfully.\")\n",
        "else:\n",
        "    logger.error(\"Failed to load OpenAI API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using Qdrant, Hugging Face, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and Hugging Face to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs - which will serve as our data for today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `SitemapLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 224/224 [00:06<00:00, 35.98it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4970"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Initializing embedding model...\n",
            "INFO:root:Embedding model initialized.\n",
            "INFO:root:Generating embeddings for documents...\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:root:Embeddings generated in 23.04 seconds.\n",
            "INFO:root:Creating Qdrant vector store...\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:root:Qdrant vector store created in 46.06 seconds.\n",
            "INFO:root:Total time taken: 69.46 seconds.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import logging\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize the embedding model\n",
        "logging.info(\"Initializing embedding model...\")\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "logging.info(\"Embedding model initialized.\")\n",
        "\n",
        "# Measure time taken to generate embeddings\n",
        "embedding_start_time = time.time()\n",
        "logging.info(\"Generating embeddings for documents...\")\n",
        "\n",
        "# Assuming the correct method is embed_documents\n",
        "embeddings = embedding_model.embed_documents([doc.page_content for doc in split_chunks])\n",
        "\n",
        "embedding_end_time = time.time()\n",
        "logging.info(f\"Embeddings generated in {embedding_end_time - embedding_start_time:.2f} seconds.\")\n",
        "\n",
        "# Measure time taken to create Qdrant vector store\n",
        "qdrant_start_time = time.time()\n",
        "logging.info(\"Creating Qdrant vector store...\")\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=split_chunks,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")\n",
        "qdrant_end_time = time.time()\n",
        "logging.info(f\"Qdrant vector store created in {qdrant_end_time - qdrant_start_time:.2f} seconds.\")\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time:.2f} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"SBTEST_{unique_id}\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "You are an AI assistant designed to answer questions based solely on the provided context. Your task is to:\n",
        "\n",
        "1. Carefully read and understand the given context.\n",
        "2. Analyze the user's question.\n",
        "3. If the context contains information relevant to answering the question, provide a clear and concise answer based only on that context.\n",
        "4. If the context does not contain information relevant to the question, or if the context is unrelated to the query, respond with \"I don't have enough relevant information to answer this question.\"\n",
        "\n",
        "Do not use any external knowledge or make assumptions beyond what is provided in the context. Be sure to maintain accuracy and relevance in your responses.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4o` in this case - below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'BaseChatModel' from 'langchain.chat_models' (c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chat_models\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRetriever\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatModel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'BaseChatModel' from 'langchain.chat_models' (c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chat_models\\__init__.py)"
          ]
        }
      ],
      "source": [
        "from langchain.schema.runnable import Runnable\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import BaseChatModel\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "def create_rag_chain(\n",
        "    retriever: BaseRetriever,\n",
        "    prompt: ChatPromptTemplate,\n",
        "    llm: BaseChatModel\n",
        ") -> Runnable:\n",
        "    \"\"\"\n",
        "    Creates a Retrieval Augmented Generation (RAG) chain.\n",
        "\n",
        "    This chain performs the following steps:\n",
        "    1. Retrieves context based on the input question\n",
        "    2. Combines the context with the original question\n",
        "    3. Sends the combined input to an LLM for response generation\n",
        "\n",
        "    Args:\n",
        "    retriever: The retriever to use for fetching relevant context\n",
        "    prompt: The prompt template to use for formatting input to the LLM\n",
        "    llm: The language model to use for generating the final response\n",
        "\n",
        "    Returns:\n",
        "    A Runnable representing the RAG chain\n",
        "    \"\"\"\n",
        "    def retrieve_context(input_dict: dict) -> dict:\n",
        "        question = input_dict[\"question\"]\n",
        "        context = retriever.get_relevant_documents(question)\n",
        "        return {\"context\": context, \"question\": question}\n",
        "\n",
        "    def generate_response(input_dict: dict) -> dict:\n",
        "        formatted_prompt = prompt.format(**input_dict)\n",
        "        response = llm.predict(formatted_prompt)\n",
        "        return {\"response\": response, \"context\": input_dict[\"context\"]}\n",
        "\n",
        "    chain = (\n",
        "        RunnablePassthrough(retrieve_context)\n",
        "        | RunnablePassthrough(generate_response)\n",
        "    )\n",
        "\n",
        "    return chain\n",
        "\n",
        "# Create the RAG chain\n",
        "retrieval_augmented_qa_chain: Runnable = create_rag_chain(\n",
        "    retriever=qdrant_retriever,\n",
        "    prompt=base_rag_prompt,\n",
        "    llm=base_llm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "4235f7af-4430-4c08-d94d-99e482ef30af"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What's new in LangChain v0.2?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "e1633e4f-5405-46c3-f344-ffc324ac69d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChain v0.2 includes several new features and improvements:\\n\\n1. Full separation of langchain and langchain-community.\\n2. New and versioned documentation.\\n3. A more mature and controllable agent framework.\\n4. Improved LLM interface standardization, particularly around tool calling.\\n5. Streaming support.\\n6. Over 30 new partner packages.\\n\\nThis release builds upon the foundation laid in v0.1 and incorporates community feedback.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FSZFdCM5LFoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content='Four months ago, we released the first stable version of LangChain. Today, we are following up by announcing a pre-release of langchain v0.2.This release builds upon the foundation laid in v0.1 and incorporates community feedback. Weâ€™re excited to share that v0.2 brings:Â A much-desired full separation of langchain and langchain-communityÂ New (and versioned!) docsÂ A more mature and controllable agent frameworkÂ Improved LLM interface standardization, particularly around tool callingBetter' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': 'debcbf42701e48fea4ea9940b8b56ebd', '_collection_name': 'ee3ecc3423174d4abddc2672d7ed99bf'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.2: A Leap Towards Stability\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.2: A Leap Towards Stability\n",
            "Today, we're announcing the pre-release of LangChain v0.2, which improves the stability and security of LangChain.\n",
            "\n",
            "5 min read\n",
            "May 10, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '0f41db711ba545b7842caf37bd7a4415', '_collection_name': 'ee3ecc3423174d4abddc2672d7ed99bf'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.1.0\n",
            "\n",
            "By LangChain\n",
            "10 min read\n",
            "Jan 8, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': '49e31ba4077c44ad959a3477162ad53c', '_collection_name': 'ee3ecc3423174d4abddc2672d7ed99bf'}\n",
            "----\n",
            "Context:\n",
            "page_content='streaming support30+ new partner packages.This is just a pre-release, with the full v0.2 release coming in a few weeks. Letâ€™s dive into what langchain v0.2 will include.Embracing stability: The evolution of LangChain architectureOne of the most notable changes in langchain v0.2 is the decoupling of the langchain package from langchain-community. As a result, langchain-community now depends on langchain-core and langchain. This is a continuation of the work we began with langchain v0.1.0 to' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '1b63b5997ac94be4be2a9a4ed5a57815', '_collection_name': 'ee3ecc3423174d4abddc2672d7ed99bf'}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "1478d061-7129-4b14-c717-c99d130a9488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't have enough relevant information to answer this question.\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:.env file found at: c:\\src\\mlops\\sb-aie4\\.env\n",
            "INFO:__main__:OpenAI API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = Path.cwd()\n",
        "\n",
        "# Construct the path to the .env file\n",
        "env_path = current_dir.parent.parent / '.env'\n",
        "\n",
        "# Check if the .env file exists\n",
        "if env_path.exists():\n",
        "    logger.info(f\".env file found at: {env_path}\")\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv(dotenv_path=env_path)\n",
        "else:\n",
        "    logger.error(f\".env file not found at: {env_path}\")\n",
        "\n",
        "# Retrieve the OpenAI API key from environment variables\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Verify that the API key is set\n",
        "if openai.api_key:\n",
        "    logger.info(\"OpenAI API key loaded successfully.\")\n",
        "else:\n",
        "    logger.error(\"Failed to load OpenAI API key.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"SBTEST\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "#os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "true\n",
            "SBTEST_10be9473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangSmith is a framework built on the shoulders of LangChain, designed to track and improve the performance of AI models. It offers features like an SDK that provides fine-grain controls and customizability, enabling developers to implement observability in their development lifecycle for LLM-related functions.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 1021, 'total_tokens': 1078, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-e0d62be3-2bd8-455e-82a0-a582b1b6cf5b-0', usage_metadata={'input_tokens': 1021, 'output_tokens': 57, 'total_tokens': 1078})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
        "print(os.getenv(\"LANGCHAIN_PROJECT\"))\n",
        "#print(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Loading Our Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJ8uCbT7S1Z",
        "outputId": "cdcbe030-aae2-46da-8cb7-30811da9f87d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DataRepository' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "w5fmy5Gy7X03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"DataRepository/langchain_blog_test_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [
        {
          "ename": "LangSmithConflictError",
          "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\utils.py:145\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\client.py:785\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    776\u001b[0m         method,\n\u001b[0;32m    777\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_kwargs,\n\u001b[0;32m    784\u001b[0m     )\n\u001b[1;32m--> 785\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\utils.py:147\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mLangSmithConflictError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[0;32m      5\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangsmith-demo-dataset-aie4-triples-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLangChain Blog Test Questions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m test_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     12\u001b[0m   triplet \u001b[38;5;241m=\u001b[39m triplet[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\client.py:2612\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[1;34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, metadata)\u001b[0m\n\u001b[0;32m   2609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2610\u001b[0m     dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_schema_definition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs_schema\n\u001b[1;32m-> 2612\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2618\u001b[0m ls_utils\u001b[38;5;241m.\u001b[39mraise_for_status_with_text(response)\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[0;32m   2621\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   2622\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[0;32m   2623\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[0;32m   2624\u001b[0m )\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\client.py:825\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    823\u001b[0m     )\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithConflictError(\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithError(\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in LangSmith\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m     )\n",
            "\u001b[1;31mLangSmithConflictError\u001b[0m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-aie4-triples-v3\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "  triplet = triplet[1]\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"], \"context\": triplet[\"context\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "## Task 6: Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_context_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.inputs[\"context\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "501c5e204387460e8e9a585c9b2ca834",
            "323a077888c84bdfbac393d1ebef9d6c",
            "710fe93f3241401bafa10a1e0a1bda02",
            "c65a6921701742da9b67c591b19b6336",
            "17d4193e78a14050a11c5f8306b3ba0b",
            "99751d16888640078ae4820cacab5760",
            "a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "54e7ccad2e774261a3313316a1397f66",
            "41e287b8dc674f839b6485c9606207d5",
            "f7457f38948d472da4027ba1566b47e4",
            "af61b863014d4a8c960570de31a02b3f"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "872581e4-9c21-414f-d25e-9454c47c0587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-f186349c' at:\n",
            "https://smith.langchain.com/o/bdfb057b-3c54-550b-8976-4c06e5176a04/datasets/a7068a1f-8d09-4fa4-ae43-de3ffb370b44/compare?selectedSessions=db7edb64-399f-4d90-84c8-46cacc92fd95\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da0396672c13472da1f4655cbcbb424e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.890000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.458000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 6.042000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 4.902000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 4.542000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 6.510000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 4.506000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.484000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 4.122000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.922000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.030000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.144000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.186000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 32e55713-37aa-426d-b866-1c1f9b0f55b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9978, Requested 356. Please try again in 2.004s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9978, Requested 356. Please try again in 2.004s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.034000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.860000 seconds\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.770000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.314000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.276000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.440000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.977000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.618000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.212000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.624000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run e383f8a3-993a-442e-b141-09c2219bfc75: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9709, Requested 377. Please try again in 516ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9709, Requested 377. Please try again in 516ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.224000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run dc5ac9ab-748f-4de0-b7a3-c3c7c3d9e326: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9696, Requested 1000. Please try again in 4.176s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9696, Requested 1000. Please try again in 4.176s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.570000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.198000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.212000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 32e55713-37aa-426d-b866-1c1f9b0f55b3: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9972, Requested 363. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9972, Requested 363. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run b1df83d3-05ba-443b-b4e5-3ae08c3faf89: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9961, Requested 378. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9961, Requested 378. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.212000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.872000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.974000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 8a976897-7b39-46ee-82e1-098f96e327bd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9922, Requested 418. Please try again in 2.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9922, Requested 418. Please try again in 2.04s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 5b0366cb-75dc-4796-911d-952a21d6e61e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9921, Requested 934. Please try again in 5.13s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9921, Requested 934. Please try again in 5.13s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.952000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.214000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.980000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 9.372000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.866000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 10.176000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.726000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run e383f8a3-993a-442e-b141-09c2219bfc75: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9717, Requested 404. Please try again in 726ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9717, Requested 404. Please try again in 726ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 6.402000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 7ba3dfc9-ae07-45e5-b8c3-ac3fc7a14bd4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9683, Requested 440. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9683, Requested 440. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.792000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.726000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.962000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.608000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.916000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 5b0366cb-75dc-4796-911d-952a21d6e61e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9887, Requested 465. Please try again in 2.112s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9887, Requested 465. Please try again in 2.112s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.100000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.136000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.202000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.164000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.900000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run d7f793f9-eb2a-4d65-8436-b944dcbdb147: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9831, Requested 376. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9831, Requested 376. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.658000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.968000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.428000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 7ba3dfc9-ae07-45e5-b8c3-ac3fc7a14bd4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9737, Requested 469. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9737, Requested 469. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.466000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.242000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 728ecc58-3741-4dbf-85cf-4f16a3deb653: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9698, Requested 345. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9698, Requested 345. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.356000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.258000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 8a976897-7b39-46ee-82e1-098f96e327bd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9621, Requested 586. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9621, Requested 586. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 6.810000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 8a30eb97-3659-4348-a573-fbaf5040344b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9959, Requested 1474. Please try again in 8.598s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9959, Requested 1474. Please try again in 8.598s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.202000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run bfe1d034-e080-4777-a225-dde4631b5717: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9936, Requested 1495. Please try again in 8.586s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9936, Requested 1495. Please try again in 8.586s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.985000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.106000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 53d73067-4eeb-403d-bd78-29c95b678c81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9907, Requested 1527. Please try again in 8.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9907, Requested 1527. Please try again in 8.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.740000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 5b0366cb-75dc-4796-911d-952a21d6e61e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9851, Requested 500. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9851, Requested 500. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 6.450000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.100000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.112000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.124000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.310000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.040000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.322000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run f9e0c392-5a6e-46f5-b876-2a4924a5503d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9945, Requested 441. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9945, Requested 441. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 606dca6d-e16c-4439-a320-68249b1272d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9859, Requested 524. Please try again in 2.298s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9859, Requested 524. Please try again in 2.298s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.386000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c6100d17-85cc-4301-b9fd-55a643afd223: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9766, Requested 619. Please try again in 2.31s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9766, Requested 619. Please try again in 2.31s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 30f6491d-6f93-46d9-a2d4-492fcddeddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9762, Requested 1714. Please try again in 8.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9762, Requested 1714. Please try again in 8.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.834000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.696000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 91c13c59-6c40-44c7-9c0b-7f01f0fe6d06: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9698, Requested 688. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9698, Requested 688. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.762000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.402000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.396000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.360000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run bfe1d034-e080-4777-a225-dde4631b5717: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9658, Requested 410. Please try again in 408ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9658, Requested 410. Please try again in 408ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 8a30eb97-3659-4348-a573-fbaf5040344b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9643, Requested 424. Please try again in 402ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9643, Requested 424. Please try again in 402ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.503000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.630000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.402000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 606dca6d-e16c-4439-a320-68249b1272d5: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9980, Requested 386. Please try again in 2.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9980, Requested 386. Please try again in 2.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run aad8168c-b644-412c-8668-47faba8abe3a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9980, Requested 1822. Please try again in 10.812s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9980, Requested 1822. Please try again in 10.812s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c6100d17-85cc-4301-b9fd-55a643afd223: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9978, Requested 387. Please try again in 2.19s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9978, Requested 387. Please try again in 2.19s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 3.072000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.220000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.118000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 97bbaafb-0569-4c1c-a143-72115de43d00: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9956, Requested 1477. Please try again in 8.598s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9956, Requested 1477. Please try again in 8.598s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 3.216000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 91c13c59-6c40-44c7-9c0b-7f01f0fe6d06: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9918, Requested 444. Please try again in 2.172s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9918, Requested 444. Please try again in 2.172s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.166000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.286000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.190000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 5.082000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.272000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c8730471-9bbd-4d4a-ac10-fa456dbe497b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9596, Requested 1250. Please try again in 5.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9596, Requested 1250. Please try again in 5.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.268000 seconds\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.166000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run bfe1d034-e080-4777-a225-dde4631b5717: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9931, Requested 446. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9931, Requested 446. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.262000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 8a30eb97-3659-4348-a573-fbaf5040344b: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9887, Requested 479. Please try again in 2.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9887, Requested 479. Please try again in 2.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.226000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.250000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.860000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.080000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c6100d17-85cc-4301-b9fd-55a643afd223: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9771, Requested 409. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9771, Requested 409. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 91c13c59-6c40-44c7-9c0b-7f01f0fe6d06: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9728, Requested 477. Please try again in 1.23s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9728, Requested 477. Please try again in 1.23s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run e711b8a4-3546-446e-b938-c35918d2dd60: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9688, Requested 1366. Please try again in 6.324s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9688, Requested 1366. Please try again in 6.324s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.438000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run aad8168c-b644-412c-8668-47faba8abe3a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9654, Requested 549. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9654, Requested 549. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.374000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 97bbaafb-0569-4c1c-a143-72115de43d00: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9612, Requested 594. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9612, Requested 594. Please try again in 1.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 3.762000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.262000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.424000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.844000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run a9188a25-cf85-49e2-a1ef-1f4269f56a15: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9826, Requested 1164. Please try again in 5.94s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9826, Requested 1164. Please try again in 5.94s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.704000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.250000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 53d73067-4eeb-403d-bd78-29c95b678c81: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9953, Requested 432. Please try again in 2.31s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9953, Requested 432. Please try again in 2.31s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.310000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.316000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c7f40cc2-b644-4d73-927f-db715cb6846e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9847, Requested 1543. Please try again in 8.34s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\qa\\eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9847, Requested 1543. Please try again in 8.34s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.094000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run aad8168c-b644-412c-8668-47faba8abe3a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9794, Requested 592. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9794, Requested 592. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.304000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 30f6491d-6f93-46d9-a2d4-492fcddeddc4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9946, Requested 514. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9946, Requested 514. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.676000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run 97bbaafb-0569-4c1c-a143-72115de43d00: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9824, Requested 636. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9824, Requested 636. Please try again in 2.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.392000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c7f40cc2-b644-4d73-927f-db715cb6846e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9647, Requested 516. Please try again in 977ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9647, Requested 516. Please try again in 977ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.506000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 1.518000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.412000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "ERROR:langsmith.evaluation._runner:Error running evaluator <DynamicRunEvaluator evaluate> on run c7f40cc2-b644-4d73-927f-db715cb6846e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9793, Requested 621. Please try again in 2.484s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1344, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 646, in wrapper\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 643, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\evaluation\\scoring\\eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 180, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
            "    raise e\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 686, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 704, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\openai\\_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-XjTfBzeM2AYGxaBNKYOlPlZz on tokens per min (TPM): Limit 10000, Used 9793, Requested 621. Please try again in 2.484s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 3.060000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.804000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_context_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    retrieval_augmented_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### â“Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17d4193e78a14050a11c5f8306b3ba0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323a077888c84bdfbac393d1ebef9d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99751d16888640078ae4820cacab5760",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "value": ""
          }
        },
        "41e287b8dc674f839b6485c9606207d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "501c5e204387460e8e9a585c9b2ca834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_323a077888c84bdfbac393d1ebef9d6c",
              "IPY_MODEL_710fe93f3241401bafa10a1e0a1bda02",
              "IPY_MODEL_c65a6921701742da9b67c591b19b6336"
            ],
            "layout": "IPY_MODEL_17d4193e78a14050a11c5f8306b3ba0b"
          }
        },
        "54e7ccad2e774261a3313316a1397f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "710fe93f3241401bafa10a1e0a1bda02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e7ccad2e774261a3313316a1397f66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e287b8dc674f839b6485c9606207d5",
            "value": 1
          }
        },
        "99751d16888640078ae4820cacab5760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f5f4beaa8f4a3b94002c5ed122c2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af61b863014d4a8c960570de31a02b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65a6921701742da9b67c591b19b6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7457f38948d472da4027ba1566b47e4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af61b863014d4a8c960570de31a02b3f",
            "value": "â€‡23/?â€‡[02:10&lt;00:00,â€‡â€‡5.36s/it]"
          }
        },
        "f7457f38948d472da4027ba1566b47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
