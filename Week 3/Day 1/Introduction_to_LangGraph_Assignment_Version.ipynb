{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:.env file found at: c:\\src\\mlops\\sb-aie4\\.env\n",
            "INFO:__main__:OpenAI API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = Path.cwd()\n",
        "\n",
        "# Construct the path to the .env file\n",
        "env_path = current_dir.parent.parent / '.env'\n",
        "\n",
        "# Check if the .env file exists\n",
        "if env_path.exists():\n",
        "    logger.info(f\".env file found at: {env_path}\")\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv(dotenv_path=env_path)\n",
        "else:\n",
        "    logger.error(f\".env file not found at: {env_path}\")\n",
        "\n",
        "# Retrieve the OpenAI API key from environment variables\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Verify that the API key is set\n",
        "if openai.api_key:\n",
        "    logger.info(\"OpenAI API key loaded successfully.\")\n",
        "else:\n",
        "    logger.error(\"Failed to load OpenAI API key.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No LangSmith API key provided. Some functionality may be limited.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from uuid import uuid4\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "\n",
        "# Try to get the API key from the environment variable\n",
        "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "# Set the LANGCHAIN_API_KEY environment variable\n",
        "if langsmith_api_key:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
        "    print(\"LangSmith API key set successfully.\")\n",
        "else:\n",
        "    print(\"No LangSmith API key provided. Some functionality may be limited.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "The model determines which tools to use based on analyzing the task at hand and matching it to the most appropriate tool(s) from those available in the tool_belt that has been bound to it. The model's choice is limited to and informed by the specific set of tools we have provided through this binding process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "**Is there any specific limit to how many times we can cycle?**\n",
        "\n",
        "*In the current implementation, there is no explicit limit to the number of cycles. The graph will continue to cycle between the \"agent\" and \"action\" nodes as long as the should_continue function keeps returning \"action\". This means the conversation could potentially go on indefinitely if the model keeps requesting tool actions.*\n",
        "\n",
        "**If not, how could we impose a limit to the number of cycles?**\n",
        "\n",
        "*To impose a limit on the number of cycles, we could modify the code and introduce a MAX_CYCLES constant to set the maximum number of allowed cycles.By implementing this, you ensure that the conversation will end after a certain number of cycles, preventing potential infinite loops or excessively long conversations.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_argCvBDDWF0vQbEZOoS3ILyx', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5796ac6771', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0b5e4c32-3e2a-4f71-974c-891555a13002-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_argCvBDDWF0vQbEZOoS3ILyx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Prior to the start of the 2023-24 season, Adam Lowry was named the new captain of the Winnipeg Jets. Responses were a bit mixed. ... Lowry is the second longest-tenured Jets player on the current ... This article was published 06/11/2023 (314 days ago), so information in it may no longer be current. ... It's been one thing after another for the new Winnipeg Jets captain, who is still getting ... In some unfortunate news for the Winnipeg Jets, ... Reflecting on Adam Lowry's First Season as Jets Captain. ... 4 Non-Playoff Teams From 2023-24 That Can Win the 2025 Stanley Cup. Here are 23 thoughts on 23 players, all on the Winnipeg Jets opening night roster ahead of Game 1 of the 2023-24 regular season in Calgary on Wednesday. Forward 1. Brad Marchand was named captain of the Boston Bruins prior to the start of the 2023-24 NHL season. ... Here is the list of each team's current captain: Anaheim Ducks: Radko ... Winnipeg Jets: Adam ...\", name='duckduckgo_search', tool_call_id='call_argCvBDDWF0vQbEZOoS3ILyx')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets, as of the 2023-24 season, is Adam Lowry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 417, 'total_tokens': 441, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5796ac6771', 'finish_reason': 'stop', 'logprobs': None}, id='run-0ba1e306-92a7-41c6-b761-ba528e88b8d6-0', usage_metadata={'input_tokens': 417, 'output_tokens': 24, 'total_tokens': 441})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_oD9sBdZbg96rO2gIq9a5cTpT', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_Ui8oEZ81xn4nw4Xer5UTgvIz', 'function': {'arguments': '{\"query\": \"latest tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 173, 'total_tokens': 223, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5796ac6771', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7c79795c-756d-4abb-96a9-18c850c20a25-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_oD9sBdZbg96rO2gIq9a5cTpT', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest tweet'}, 'id': 'call_Ui8oEZ81xn4nw4Xer5UTgvIz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 50, 'total_tokens': 223})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_oD9sBdZbg96rO2gIq9a5cTpT'), ToolMessage(content=\"The campaign for Vice President and Democratic nominee for President Kamala Harris posted a video of the moment on X (formerly Twitter), with a caption reciting Trump's comments. NEW YORK (AP) ‚Äî Donald Trump shared more than a dozen posts on his social media network Wednesday that call for the trial or jailing of House lawmakers who investigated the attack on the U.S. Capitol, special counsel Jack Smith and others, along with images that reference the QAnon conspiracy theory. The former president began posting a string of messages Tuesday evening after Smith filed a ... Elon Musk Writes, Then Deletes, a Post Musing About Threats to Biden and Harris. His remark, just hours after what the authorities said was a second assassination attempt on Donald J. Trump ... In more serious matters, EU Commissioner Thierry Breton visited Twitter's headquarters to conduct a stress test for the new Digital Services Act regulating everything from social media content ... Learn how to use Twitter's Advanced Search page or inline parameters to find tweets posted within a certain date range. You can also sort the results by relevance, date, or media type.\", name='duckduckgo_search', tool_call_id='call_Ui8oEZ81xn4nw4Xer5UTgvIz')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Qrfbb97HG82gzrfccLzWeJA3', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_ZSXEgWyYUdUEBe5JPv0Y2eVd', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_lzzK8AWAMl9uuAI6KLd3ON0v', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_5lGtpJ93bT4cBp1lLXw9QEku', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 1421, 'total_tokens': 1529, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5796ac6771', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c0525439-64df-44de-91b1-778162b527b2-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest tweet'}, 'id': 'call_Qrfbb97HG82gzrfccLzWeJA3', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest tweet'}, 'id': 'call_ZSXEgWyYUdUEBe5JPv0Y2eVd', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest tweet'}, 'id': 'call_lzzK8AWAMl9uuAI6KLd3ON0v', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest tweet'}, 'id': 'call_5lGtpJ93bT4cBp1lLXw9QEku', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1421, 'output_tokens': 108, 'total_tokens': 1529})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: duckduckgo_search\n",
            "[ToolMessage(content='But Hixson\\'s ruling said Meta researcher Tim Dettmers, who was talking with other researchers from the nonprofit group EleutherAI on the instant messaging platform Discord, didn\\'t have the authority to waive attorney-client privilege. In the chat logs, researcher Tim Dettmers describes his back-and-forth with Meta\\'s legal department over whether use of the book files as training data would be \"legally ok.\" \"At Facebook ... In the chat quoted in the complaint, researcher Tim Dettmers talks about his back-and-forth with Meta\\'s legal department whether the use of the book files as training data would be \"legally ok ... In the chat logs quoted in the complaint, researcher Tim Dettmers describes his back-and-forth with Meta\\'s legal department over whether use of the book files as training data would be \"legally ok.\" In late 2020, Meta researcher Tim Dettmers expressed interest in using The Pile in a conversation on the EleutherAI public Discord server and asked about \"any legal concerns\" with using the ...', name='duckduckgo_search', tool_call_id='call_Qrfbb97HG82gzrfccLzWeJA3'), ToolMessage(content=\"We also touch upon the latest research advances that attempts to utilize causal only decoder models to learn better dense representations, which as we will see in our work, is also suitable for learned sparse retrievers. ... Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various ... In the experiments, we employ the latest version of GPT-3.5 (gpt-3.5-turbo-0125) as the base model. ... Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. There has been tremendous interest from both researchers and the general public about the latest advancements in large-scale language models ... Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.\", name='duckduckgo_search', tool_call_id='call_ZSXEgWyYUdUEBe5JPv0Y2eVd'), ToolMessage(content=\"Error: RatelimitException('https://duckduckgo.com 202 Ratelimit')\\n Please fix your mistakes.\", name='duckduckgo_search', tool_call_id='call_lzzK8AWAMl9uuAI6KLd3ON0v'), ToolMessage(content=\"Error: RatelimitException('https://duckduckgo.com 202 Ratelimit')\\n Please fix your mistakes.\", name='duckduckgo_search', tool_call_id='call_5lGtpJ93bT4cBp1lLXw9QEku')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='I found the QLoRA paper on Arxiv, titled \"QLoRA: Efficient Finetuning of Quantized LLMs,\" authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Here is a brief summary:\\n\\n- **Title**: QLoRA: Efficient Finetuning of Quantized LLMs\\n- **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\n- **Summary**: QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. The paper introduces several innovations to save memory without sacrificing performance, such as 4-bit NormalFloat (NF4), double quantization, and paged optimizers. The authors provide a detailed analysis of instruction following and chatbot performance across various datasets and model scales.\\n\\nFor the latest tweets of the authors, I encountered some issues:\\n\\n- **Tim Dettmers**: I found some information about his involvement in discussions on the EleutherAI public Discord server, but no specific latest tweet.\\n- **Artidoro Pagnoni**: No specific latest tweet found.\\n- **Ari Holtzman**: Encountered a rate limit error.\\n- **Luke Zettlemoyer**: Encountered a rate limit error.\\n\\nWould you like me to attempt another search for the latest tweets of Ari Holtzman and Luke Zettlemoyer?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 2201, 'total_tokens': 2528, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_057232b607', 'finish_reason': 'stop', 'logprobs': None}, id='run-4b718394-67e5-4d39-8d20-33cb69bb0ebc-0', usage_metadata={'input_tokens': 2201, 'output_tokens': 327, 'total_tokens': 2528})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "# Agent's Steps to Process the Query\n",
        "\n",
        "1. **Initial Query Processing**\n",
        "   - Received human message: \"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\"\n",
        "\n",
        "2. **Arxiv Search**\n",
        "   - Used Arxiv tool to search for the QLoRA paper\n",
        "   - Found information about \"QLoRA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer\n",
        "\n",
        "3. **Author Identification**\n",
        "   - Identified authors of the QLoRA paper from Arxiv search results\n",
        "\n",
        "4. **DuckDuckGo Searches**\n",
        "   - Performed DuckDuckGo search for each author\n",
        "   - Retrieved general information about authors and their work\n",
        "\n",
        "5. **Result Compilation**\n",
        "   - Compiled search results from both Arxiv and DuckDuckGo\n",
        "\n",
        "6. **Attempt to Find Tweets**\n",
        "   - Attempted to find tweets from authors using DuckDuckGo search results\n",
        "   - Unsuccessful in retrieving actual tweet content\n",
        "\n",
        "7. **Response Formulation**\n",
        "   - Likely formulated a response indicating:\n",
        "     - Found information about QLoRA paper\n",
        "     - Retrieved general information about authors\n",
        "     - Unable to locate specific tweets\n",
        "\n",
        "8. **Output Generation**\n",
        "   - Generated output for each step\n",
        "   - Showed tools used (Arxiv and DuckDuckGo) and content retrieved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model alone might not have enough information to produce a high-quality response, such as in open-domain question answering, dialogue systems, and other applications requiring access to a large knowledge base.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the main contribution of the QLoRA paper?\",\n",
        "    \"How does QLoRA reduce memory usage compared to full fine-tuning?\",\n",
        "    \"What is NF4, as introduced in the QLoRA paper?\",\n",
        "    \"How does QLoRA compare to other efficient fine-tuning methods in terms of performance?\",\n",
        "    \"What is the significance of the Guanaco model mentioned in the QLoRA paper?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"efficient finetuning\", \"quantized LLMs\", \"memory reduction\"]},\n",
        "    {\"must_mention\": [\"4-bit quantization\", \"Low Rank Adapters\", \"frozen pretrained model\"]},\n",
        "    {\"must_mention\": [\"4-bit\", \"NormalFloat\", \"optimal for normal distribution\"]},\n",
        "    {\"must_mention\": [\"full 16-bit performance\", \"less memory\", \"faster training\"]},\n",
        "    {\"must_mention\": [\"outperforms previous models\", \"Vicuna benchmark\", \"single GPU training\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "# How Questions and Answers are Associated\n",
        "\n",
        "1. **Indexing Correspondence**\n",
        "   - Questions and answers are stored in separate lists: `questions` and `answers`.\n",
        "   - Each question corresponds to the answer at the same index in the `answers` list.\n",
        "\n",
        "2. **Answer Format**\n",
        "   - Answers are not direct responses, but dictionaries with a \"must_mention\" key.\n",
        "   - The \"must_mention\" value is a list of key concepts or phrases.\n",
        "\n",
        "3. **Evaluation Mechanism**\n",
        "   - Correct answers are determined by checking if the agent's response includes the phrases listed in \"must_mention\".\n",
        "\n",
        "4. **Flexibility in Evaluation**\n",
        "   - This approach allows for partial credit and flexibility in wording.\n",
        "   - The agent doesn't need to provide an exact match to a predefined answer.\n",
        "\n",
        "5. **Potential Issues**\n",
        "   - *Ambiguity*: Some key phrases might be too general or could appear in incorrect answers.\n",
        "   - *Incompleteness*: The \"must_mention\" list might not capture all aspects of a correct answer.\n",
        "   - *Lack of Context*: The evaluation doesn't consider the overall coherence or correctness of the response.\n",
        "   - *Maintenance*: Updating questions requires careful adjustment of the corresponding answer criteria.\n",
        "\n",
        "6. **Advantages**\n",
        "   - *Simplicity*: Easy to implement and understand.\n",
        "   - *Adaptability*: Can work with various types of questions and response styles.\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "# Improving the Must-Mention Metric\n",
        "\n",
        "1. **Partial Credit Scoring**\n",
        "   - Instead of binary scoring, implement a graduated scale based on the number of required phrases mentioned.\n",
        "   - Example: Score = (number of phrases mentioned) / (total required phrases)\n",
        "\n",
        "2. **Phrase Importance Weighting**\n",
        "   - Assign different weights to required phrases based on their importance.\n",
        "   - Calculate a weighted score instead of a simple all-or-nothing approach.\n",
        "\n",
        "3. **Semantic Similarity**\n",
        "   - Use word embeddings or language models to detect semantic equivalents of required phrases.\n",
        "   - This allows for more flexible matching beyond exact string matching.\n",
        "\n",
        "4. **Context Consideration**\n",
        "   - Implement a method to evaluate if the required phrases are used in the correct context.\n",
        "   - This could involve analyzing surrounding words or sentence structure.\n",
        "\n",
        "5. **Negative Scoring**\n",
        "   - Introduce penalties for mentioning incorrect or contradictory information.\n",
        "   - This helps ensure the overall accuracy of the response, not just the presence of key phrases.\n",
        "\n",
        "6. **Length Normalization**\n",
        "   - Adjust scores based on the length of the response to avoid favoring overly verbose answers.\n",
        "\n",
        "7. **Phrase Order Consideration**\n",
        "   - For some questions, the order of mentioned phrases might be important. Implement a way to consider this.\n",
        "\n",
        "# Gaps in the Current Method\n",
        "\n",
        "1. **Lack of Nuance**\n",
        "   - The current binary scoring doesn't capture partial knowledge or near-misses.\n",
        "\n",
        "2. **Over-simplification**\n",
        "   - Complex topics might be reduced to a few key phrases, missing important nuances.\n",
        "\n",
        "3. **Potential for Gaming**\n",
        "   - A model could theoretically score well by simply mentioning all required phrases without coherence.\n",
        "\n",
        "4. **Ignoring Overall Correctness**\n",
        "   - The method doesn't evaluate the overall correctness or coherence of the answer.\n",
        "\n",
        "5. **Limited to Predefined Phrases**\n",
        "   - It may not recognize novel but correct formulations of answers.\n",
        "\n",
        "6. **No Consideration of Explanation Quality**\n",
        "   - The depth or quality of explanations isn't factored into the score.\n",
        "\n",
        "7. **Inability to Handle Open-ended Questions**\n",
        "   - This method is less suitable for questions requiring creative or diverse answers.\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - b3f02c88' at:\n",
            "https://smith.langchain.com/o/bdfb057b-3c54-550b-8976-4c06e5176a04/datasets/efeb7a27-2edd-4cac-932f-a2d0fc0a46c8/compare?selectedSessions=fd3dba37-27ef-48a9-a689-9e1c9c3a4716\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 1aace49d at:\n",
            "https://smith.langchain.com/o/bdfb057b-3c54-550b-8976-4c06e5176a04/datasets/efeb7a27-2edd-4cac-932f-a2d0fc0a46c8\n",
            "[------------------------------------------------->] 5/5"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4b5cf97a-44f9-4e76-8196-cc62eac9bbcb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.172188</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.816041</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.906258</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.023300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.101165</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.680836</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.149381</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      5     0        5.000000   \n",
              "unique                     1     0             NaN   \n",
              "top                    False   NaN             NaN   \n",
              "freq                       5   NaN             NaN   \n",
              "mean                     NaN   NaN        5.172188   \n",
              "std                      NaN   NaN        1.816041   \n",
              "min                      NaN   NaN        3.906258   \n",
              "25%                      NaN   NaN        4.023300   \n",
              "50%                      NaN   NaN        4.101165   \n",
              "75%                      NaN   NaN        5.680836   \n",
              "max                      NaN   NaN        8.149381   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      5  \n",
              "unique                                     5  \n",
              "top     4b5cf97a-44f9-4e76-8196-cc62eac9bbcb  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - b3f02c88',\n",
              " 'results': {'1c85c6c9-0e10-400d-a5df-89f10eb4d94b': {'input': {'question': 'What is the significance of the Guanaco model mentioned in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4fce4bce-ab32-477e-82a4-e0b8b4318e77'), target_run_id=None)],\n",
              "   'execution_time': 5.680836,\n",
              "   'run_id': '4b5cf97a-44f9-4e76-8196-cc62eac9bbcb',\n",
              "   'output': 'The Guanaco model, as mentioned in the QLoRA paper, represents a significant advancement in the efficient fine-tuning of large language models (LLMs). Here are the key points about the Guanaco model and its significance:\\n\\n1. **Efficiency in Fine-Tuning**: The QLoRA approach allows for the fine-tuning of a 65 billion parameter model on a single 48GB GPU while maintaining the performance of full 16-bit fine-tuning tasks. This is achieved by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\\n\\n2. **Performance**: The Guanaco model family, which is the best model family produced using QLoRA, outperforms all previously released open models on the Vicuna benchmark. It achieves 99.3% of the performance level of ChatGPT, requiring only 24 hours of fine-tuning on a single GPU.\\n\\n3. **Innovations**: QLoRA introduces several innovations to save memory without sacrificing performance:\\n   - **4-bit NormalFloat (NF4)**: A new data type that is theoretically optimal for normally distributed weights.\\n   - **Double Quantization**: Reduces the average memory footprint by quantizing the quantization constants.\\n   - **Paged Optimizers**: Manages memory spikes effectively.\\n\\n4. **Scalability**: QLoRA enables the fine-tuning of large models (e.g., 33B and 65B parameter models) that would be infeasible with regular fine-tuning methods. This scalability is crucial for advancing the capabilities of LLMs.\\n\\n5. **Evaluation and Analysis**: The paper provides a detailed analysis of instruction following and chatbot performance across multiple datasets and model types. It also highlights the limitations of current chatbot benchmarks and offers insights into where the Guanaco model falls short compared to ChatGPT.\\n\\n6. **Open Source Contribution**: The authors have released all models and code, including CUDA kernels for 4-bit training, contributing to the open-source community and enabling further research and development in this area.\\n\\nIn summary, the Guanaco model demonstrates the potential of QLoRA to make the fine-tuning of large language models more efficient and accessible, while achieving near state-of-the-art performance.',\n",
              "   'reference': {'must_mention': ['outperforms previous models',\n",
              "     'Vicuna benchmark',\n",
              "     'single GPU training']}},\n",
              "  '2e1ec049-bfd0-4e68-95dc-5042135f5b53': {'input': {'question': 'What is the main contribution of the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('572f4698-8bad-4c8f-bad3-228b937e4c0d'), target_run_id=None)],\n",
              "   'execution_time': 4.101165,\n",
              "   'run_id': '9b541a39-26ba-4b86-981e-bda806321a61',\n",
              "   'output': 'The main contribution of the QLoRA paper, specifically the \"IR-QLoRA\" (Information Retention Quantized Low-Rank Adaptation), is the development of a novel method to enhance the accuracy of quantized large language models (LLMs) while maintaining computational efficiency. The key innovations include:\\n\\n1. **Statistics-based Information Calibration Quantization**: This technique ensures that the quantized parameters of the LLM retain the original information accurately.\\n2. **Finetuning-based Information Elastic Connection**: This allows LoRA (Low-Rank Adaptation) to utilize elastic representation transformation with diverse information.\\n\\nThese advancements enable significant improvements in accuracy for LLaMA and LLaMA2 models under 2-4 bit-widths, with minimal additional time consumption. The method is versatile and compatible with various quantization frameworks, providing general accuracy gains. The code for IR-QLoRA is available on GitHub.',\n",
              "   'reference': {'must_mention': ['efficient finetuning',\n",
              "     'quantized LLMs',\n",
              "     'memory reduction']}},\n",
              "  '5902f636-4c0f-4773-a5f3-91112d462d0b': {'input': {'question': 'How does QLoRA reduce memory usage compared to full fine-tuning?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('27df42a5-bb94-4109-8a8d-3468afa4b4e4'), target_run_id=None)],\n",
              "   'execution_time': 3.906258,\n",
              "   'run_id': '66ca87ab-5d6d-4b36-8c0a-6a638ffb92a7',\n",
              "   'output': \"QLoRA (Quantized Low-Rank Adaptation) reduces memory usage compared to full fine-tuning by employing a combination of quantization and low-rank adaptation techniques. Here's a detailed explanation of how it achieves this:\\n\\n1. **Quantization**:\\n   - **Quantization** involves reducing the precision of the model's weights from 32-bit floating-point numbers (FP32) to lower precision formats such as 8-bit integers (INT8). This significantly reduces the memory footprint of the model.\\n   - By using lower precision, the amount of memory required to store the model's weights is reduced by a factor of 4 (since 8-bit integers are 4 times smaller than 32-bit floats).\\n\\n2. **Low-Rank Adaptation**:\\n   - **Low-Rank Adaptation** involves decomposing the weight matrices into products of smaller matrices. Instead of updating the full weight matrix during fine-tuning, QLoRA updates only the smaller matrices.\\n   - This reduces the number of parameters that need to be stored and updated during fine-tuning, further decreasing memory usage.\\n\\n3. **Combination of Both Techniques**:\\n   - By combining quantization and low-rank adaptation, QLoRA achieves a substantial reduction in memory usage. The quantized model requires less memory to store the weights, and the low-rank adaptation reduces the number of parameters that need to be fine-tuned.\\n   - This allows for efficient fine-tuning of large models on hardware with limited memory, such as GPUs with smaller VRAM.\\n\\nIn summary, QLoRA reduces memory usage by quantizing the model's weights to lower precision and by using low-rank adaptation to minimize the number of parameters that need to be fine-tuned. This combination allows for efficient fine-tuning with significantly lower memory requirements compared to full fine-tuning.\",\n",
              "   'reference': {'must_mention': ['4-bit quantization',\n",
              "     'Low Rank Adapters',\n",
              "     'frozen pretrained model']}},\n",
              "  '607e5b8e-d40e-4e00-898c-e587d13a64b0': {'input': {'question': 'How does QLoRA compare to other efficient fine-tuning methods in terms of performance?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('11148a60-8c01-4fac-9286-b689b1ca2fcc'), target_run_id=None)],\n",
              "   'execution_time': 8.149381,\n",
              "   'run_id': '7064b1b4-0448-409d-a74a-71f62cf7b3db',\n",
              "   'output': \"### QLoRA vs Other Efficient Fine-Tuning Methods: Performance Comparison\\n\\n#### Overview of QLoRA\\nQLoRA (Quantized Low-Rank Adaptation) is an efficient fine-tuning approach designed to reduce memory usage while preserving the performance of full 16-bit fine-tuning tasks. It achieves this by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Key innovations include:\\n- **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\\n- **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\\n- **Paged Optimizers**: Manages memory spikes effectively.\\n\\n#### Performance Highlights\\n- **Memory Efficiency**: QLoRA can fine-tune a 65B parameter model on a single 48GB GPU.\\n- **Performance**: Achieves 99.3% of ChatGPT's performance on the Vicuna benchmark with only 24 hours of fine-tuning on a single GPU.\\n- **Versatility**: Effective across various model types (e.g., LLaMA, T5) and scales (e.g., 33B and 65B parameter models).\\n\\n#### Comparison with Other Methods\\n1. **LoRA (Low-Rank Adaptation)**:\\n   - **Memory Usage**: QLoRA is more memory-efficient due to its quantization techniques.\\n   - **Performance**: Both methods maintain high performance, but QLoRA's quantization offers better performance with large datasets.\\n   - **Training Speed**: QLoRA might be slightly quicker due to better resource management.\\n\\n2. **IR-QLoRA (Information Retention QLoRA)**:\\n   - **Accuracy**: IR-QLoRA improves accuracy by retaining original information through statistics-based Information Calibration Quantization and finetuning-based Information Elastic Connection.\\n   - **Efficiency**: Achieves significant performance gains with minimal additional time consumption (e.g., 4-bit LLaMA-7B achieves 1.4% improvement on MMLU with only 0.31% additional time).\\n\\n3. **Full Fine-Tuning**:\\n   - **Resource Requirements**: Full fine-tuning requires significantly more resources and costs compared to parameter-efficient techniques like QLoRA.\\n   - **Performance**: While full fine-tuning can improve model performance on specific tasks, QLoRA achieves similar performance with fewer trainable parameters and lower resource consumption.\\n\\n#### Conclusion\\nQLoRA stands out as a highly efficient fine-tuning method that balances memory usage, performance, and training speed. Its quantization techniques make it particularly suitable for environments with limited VRAM, and it performs exceptionally well on large datasets. Compared to other methods like LoRA and full fine-tuning, QLoRA offers a more resource-efficient alternative without compromising on performance.\",\n",
              "   'reference': {'must_mention': ['full 16-bit performance',\n",
              "     'less memory',\n",
              "     'faster training']}},\n",
              "  '90e4a0dc-b334-4e65-acd6-516c4ed0fd52': {'input': {'question': 'What is NF4, as introduced in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6bb62089-76fe-4f94-a68a-3c416a6fa0c5'), target_run_id=None)],\n",
              "   'execution_time': 4.0233,\n",
              "   'run_id': '2ae20dfa-fd21-4cf6-b3ac-9192722c436f',\n",
              "   'output': 'In the QLoRA paper, NF4 stands for \"4-bit NormalFloat,\" which is a new data type introduced to optimize memory usage during the fine-tuning of large language models. Here are the key points about NF4:\\n\\n1. **Purpose**: NF4 is designed to be information-theoretically optimal for representing normally distributed weights, which are common in pretrained language models.\\n\\n2. **Memory Efficiency**: By using NF4, QLoRA can significantly reduce the memory footprint, allowing the fine-tuning of large models (up to 65 billion parameters) on a single 48GB GPU without sacrificing performance.\\n\\n3. **Implementation**: NF4 is part of a broader set of innovations in QLoRA, which also includes double quantization and paged optimizers to manage memory spikes.\\n\\n4. **Performance**: The use of NF4 in QLoRA enables the fine-tuning of models to achieve high performance levels, comparable to full 16-bit fine-tuning, while being much more memory efficient.\\n\\nIn summary, NF4 is a crucial component of the QLoRA approach, enabling efficient and effective fine-tuning of large language models by optimizing memory usage through a specialized 4-bit quantization scheme.',\n",
              "   'reference': {'must_mention': ['4-bit',\n",
              "     'NormalFloat',\n",
              "     'optimal for normal distribution']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating a State Graph with Helpfulness Check\n",
        "\n",
        "This code cell is setting up a new graph structure for an AI agent with a helpfulness check. Let's break it down:\n",
        "\n",
        "1. **Initializing the State Graph**\n",
        "   ```python\n",
        "   graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "   ```\n",
        "   - Creates a new `StateGraph` object named `graph_with_helpfulness_check`\n",
        "   - Uses `AgentState` as the state type for this graph\n",
        "\n",
        "2. **Adding the Agent Node**\n",
        "   ```python\n",
        "   graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "   ```\n",
        "   - Adds a node named \"agent\" to the graph\n",
        "   - Associates this node with the `call_model` function\n",
        "   - This node likely represents the main AI model's decision-making process\n",
        "\n",
        "3. **Adding the Action Node**\n",
        "   ```python\n",
        "   graph_with_helpfulness_check.add_node(\"action\", tool_node)\n",
        "   ```\n",
        "   - Adds another node named \"action\" to the graph\n",
        "   - Associates this node with the `tool_node` object\n",
        "   - This node probably handles the execution of specific tools or actions\n",
        "\n",
        "## Purpose\n",
        "- This graph structure allows for a more complex flow of operations in the AI agent.\n",
        "- It separates the decision-making process (agent node) from the action execution (action node).\n",
        "- The name `graph_with_helpfulness_check` suggests that this graph might include additional logic to evaluate the helpfulness of the agent's responses.\n",
        "\n",
        "## Next Steps\n",
        "- After setting up these nodes, the graph would typically need edges defined to connect these nodes and specify the flow of operations.\n",
        "- Additional nodes or conditional logic might be added to implement the helpfulness check functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting the Entry Point for the AI Graph\n",
        "\n",
        "```python\n",
        "graph_with_helpfulness_check.set_entry_point(\"agent\")\n",
        "```\n",
        "\n",
        "## Purpose\n",
        "- Defines the starting node for the graph's execution flow.\n",
        "- Specifies \"agent\" as the initial processing point.\n",
        "\n",
        "## Implications\n",
        "- All interactions begin with the AI model's analysis.\n",
        "- Allows the model to:\n",
        "  1. Handle simple queries directly.\n",
        "  2. Decide when to use additional tools/actions.\n",
        "\n",
        "## Design Choice\n",
        "- Prioritizes AI assessment before any other operations.\n",
        "- Supports implementation of helpfulness checks early in the process.\n",
        "\n",
        "## Significance\n",
        "- Crucial for defining the operational flow of the AI system.\n",
        "- Ensures the primary AI model is the first point of contact for inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Purpose\n",
        "- Determines next action based on AI's response\n",
        "- Checks for tool calls or evaluates response helpfulness\n",
        "\n",
        "## Key Components\n",
        "1. **Tool Call Check**: Returns \"action\" if tool calls present\n",
        "2. **Message Limit Check**: Ends process if over 10 messages\n",
        "3. **Helpfulness Evaluation**: \n",
        "   - Uses GPT-4 to assess response helpfulness\n",
        "   - Compares initial query and final response\n",
        "4. **Decision Logic**:\n",
        "   - \"end\" if helpful\n",
        "   - \"continue\" if not helpful\n",
        "\n",
        "## Significance\n",
        "- Implements feedback loop for response quality\n",
        "- Balances tool use and direct answers\n",
        "- Prevents overly long conversations\n",
        "\n",
        "This function is a crucial decision point in the AI's conversation flow, determining whether to end, refine, or execute a tool action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function: tool_call_or_helpful\n",
        "\n",
        "This function is a key decision-making component in the conversation flow. Here's a breakdown of its operation:\n",
        "\n",
        "1. **Initial Check for Tool Calls**\n",
        "   - Checks if the last message contains any tool calls\n",
        "   - If yes, returns \"action\", indicating a tool should be used\n",
        "\n",
        "2. **Message Limit Check**\n",
        "   - If there are more than 10 messages, returns \"END\" to prevent overly long conversations\n",
        "\n",
        "3. **Helpfulness Evaluation**\n",
        "   - Proceeds if neither of the above conditions are met\n",
        "\n",
        "4. **Prompt Creation**\n",
        "   - Defines a prompt template to assess the helpfulness of the final response\n",
        "\n",
        "5. **Model Setup**\n",
        "   - Uses ChatOpenAI with GPT-4 model for evaluation\n",
        "\n",
        "6. **Evaluation Chain**\n",
        "   - Creates a chain: prompt_template -> GPT-4 model -> String output parser\n",
        "\n",
        "7. **Helpfulness Check**\n",
        "   - Invokes the chain with initial query and final response\n",
        "   - Expects 'Y' or 'N' response indicating helpfulness\n",
        "\n",
        "8. **Final Decision**\n",
        "   - Returns \"end\" if response contains 'Y' (conversation complete)\n",
        "   - Returns \"continue\" otherwise (for a more helpful response)\n",
        "\n",
        "This function serves as a sophisticated decision point, determining whether to:\n",
        "- Use a tool\n",
        "- End the conversation due to length\n",
        "- End it due to a satisfactory response\n",
        "- Continue for a more helpful answer\n",
        "\n",
        "It leverages another AI model (GPT-4) for helpfulness judgment, adding an extra layer of intelligence to conversation management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_anUH5b3fdzmbiNcufB8MkFRF', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_GPiErYOKrt7v47SnPjzENHx3', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_yJAATFIfvhBdoUyjo1ImMoxr', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5796ac6771', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-35aece06-09d7-430a-8971-5201391055db-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_anUH5b3fdzmbiNcufB8MkFRF', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_GPiErYOKrt7v47SnPjzENHx3', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_yJAATFIfvhBdoUyjo1ImMoxr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA (Low-Rank Adaptation) is a method that reduces the trainable parameters and GPU memory requirements of large language models like GPT-3 by adding small, changeable parts to each layer. Learn how LoRA works, why it is useful, and how it differs from traditional fine-tuning in this tutorial. LoRA's approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model's learning capability is preserved with significantly ... Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (approximately 100K prompt-response ... This is where Low-Rank Adaptation (LoRA) technique appeared to be a game-changer enabling fine-tuning very big LLMs to specific task on limited resources and datasets. LoRA introduces a seemingly-simple yet powerful and cost-effective way to fine-tune LLMs and adapt them to a specific task by integrating low-rank matrices into the model's ...\", name='duckduckgo_search', tool_call_id='call_anUH5b3fdzmbiNcufB8MkFRF'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... In the chat logs quoted in the complaint, researcher Tim Dettmers describes his back-and-forth with Meta\\'s legal department over whether use of the book files as training data would be \"legally ok.\" By Tim Dettmers, Michael Diskin, Aston Zhang, Alexander Borzunov | cohere.ai. Stay ahead of the game: Get a sneak peek of the coolest natural language processing (NLP) research of February 2023! Our handpicked selection of the best NLP papers will keep you up-to-date on the latest advancements in language models, text generation, and ... Tim Dettmers\\'s research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... In the chat quoted in the complaint, researcher Tim Dettmers talks about his back-and-forth with Meta\\'s legal department whether the use of the book files as training data would be \"legally ok ...', name='duckduckgo_search', tool_call_id='call_GPiErYOKrt7v47SnPjzENHx3'), ToolMessage(content='The attention mechanism is a technique used in machine learning and natural language processing to increase model accuracy by focusing on relevant data. It enables the model to focus on certain areas of the input data, giving more weight to crucial features and disregarding unimportant ones. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer ... The Transformer, since then, has become a popular architecture choice for a variety of tasks. It is capable of capturing long-range dependencies in data making it a powerful tool not only for NLP but also for computer vision, audio, and protein folding. Transformer: the all-powerful in the land of machine learning. Machine translation: Models like Google Translate leverage attention to focus on relevant parts of the source sentence and produce more contextually accurate translations. Text summarisation: Important sentences or phrases in a document can be found with attention, facilitating more informative and concise summaries. Although the attention mechanism is an earlier concept (Bahdanau et al., 2014, Luong et al., 2015), the paper \"Attention is all you Need\" (Vaswani et al., 2017), with its provocative title, gave it the love it deserved.The paper unveiled the transformer architecture, showcasing its prowess in machine translation and unknowingly handing the attention mechanism the keys to the AI kingdom.', name='duckduckgo_search', tool_call_id='call_yJAATFIfvhBdoUyjo1ImMoxr')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### LoRA (Low-Rank Adaptation)\\n\\nLoRA, or Low-Rank Adaptation, is a method used in machine learning to fine-tune large language models (LLMs) like GPT-3 in a more efficient manner. Traditional fine-tuning involves adjusting the pre-trained weights of a model, which can be resource-intensive. LoRA, on the other hand, reduces the number of trainable parameters and the GPU memory requirements by introducing small, changeable parts to each layer of the model. This is achieved by decomposing the weight updates into a product of lower-rank matrices, which balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency.\\n\\n### Tim Dettmers\\n\\nTim Dettmers is a researcher whose work focuses on making large foundation models, such as ChatGPT, more accessible to researchers and practitioners by reducing their resource requirements. He has developed novel compression and networking algorithms and built systems that allow for memory-efficient, fast, and cost-effective deep learning. One of his notable contributions is QLoRA, a method that significantly reduces the GPU memory required to fine-tune large models, making it possible to fine-tune a 65 billion parameter model on a single GPU.\\n\\n### Attention Mechanism\\n\\nThe attention mechanism is a technique used in machine learning and natural language processing to improve model accuracy by focusing on relevant parts of the input data. It allows the model to give more weight to crucial features and disregard less important ones. This mechanism is inspired by the human cognitive process of selective focus and has become a fundamental component in various applications, including natural language processing, computer vision, and more.\\n\\nThe attention mechanism gained significant recognition with the introduction of the Transformer architecture, as described in the paper \"Attention is All You Need\" by Vaswani et al. (2017). This architecture has proven to be highly effective in capturing long-range dependencies in data, making it a powerful tool for tasks such as machine translation, text summarization, and more.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 401, 'prompt_tokens': 1201, 'total_tokens': 1602, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5796ac6771', 'finish_reason': 'stop', 'logprobs': None}, id='run-fbe72a20-6656-4302-9e31-ba22ff0b250e-0', usage_metadata={'input_tokens': 1201, 'output_tokens': 401, 'total_tokens': 1602})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models (LLMs) like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or creative output from the model.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing prompts that are clear, specific, and tailored to the task at hand.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve the quality of the output.\n",
            "3. **Understanding Model Behavior**: Gaining insights into how the model interprets different types of prompts and adjusting accordingly.\n",
            "4. **Use Cases**: Applications range from generating creative content, answering questions, summarizing text, to more complex tasks like code generation and data analysis.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of powerful language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals. The term \"prompt engineering\" itself started gaining traction around this time as researchers and practitioners began to explore and document best practices for interacting with these models.\n",
            "\n",
            "To get more precise information on the timeline and development of prompt engineering, I can look up recent articles or papers on the topic. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The technique leverages both retrieval and generation to handle knowledge-intensive tasks more effectively than traditional methods.\n",
            "\n",
            "Would you like more detailed information or recent developments on RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large, general dataset, and refines it to perform better on a more specific task or domain. Fine-tuning is particularly useful in natural language processing (NLP) and computer vision, where large pre-trained models like BERT, GPT, and ResNet can be adapted to specific tasks such as sentiment analysis, text classification, or object detection.\n",
            "\n",
            "Fine-tuning became more prominent with the advent of transfer learning and the development of large pre-trained models. The concept of transfer learning has been around for a while, but it gained significant attention in the late 2010s with the success of models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models demonstrated that fine-tuning could achieve state-of-the-art results on a variety of NLP tasks.\n",
            "\n",
            "To get more precise information on when fine-tuning became a significant trend, I can look up relevant articles and papers. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, represent a new paradigm in artificial intelligence where large language models (LLMs) are enhanced with the ability to perceive and utilize external resources and tools. This significantly extends their versatility and expertise compared to standalone LLMs.\n",
            "\n",
            "### Key Points:\n",
            "1. **Definition**: LLM-based agents are AI systems that leverage large language models to perform complex tasks by interacting with external resources and tools.\n",
            "2. **Capabilities**: These agents can read, edit files, run commands, and utilize specific APIs, making them highly versatile in various applications.\n",
            "3. **Evolution**: The concept of LLM-based agents has evolved from traditional agent-based modeling and simulation, which has been a powerful tool for modeling complex systems.\n",
            "\n",
            "### Historical Context:\n",
            "- **Recent Advances**: The rise of powerful LLMs has spurred the development of LLM-based agents. This trend has been particularly noticeable in the last few years.\n",
            "- **Key Developments**:\n",
            "  - **2020**: Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework was introduced, which integrates the advantages of heterogeneous agents.\n",
            "  - **2023**: Studies focused on improving human-agent interactions, such as enhancing empathy towards robotic virtual agents.\n",
            "  - **2024**: EvoAgent was introduced, a method to automatically extend expert agents to multi-agent systems via evolutionary algorithms.\n",
            "\n",
            "### Notable Research:\n",
            "1. **EvoAgent (2024)**: A method to automatically extend expert agents to multi-agent systems, improving the effectiveness of LLM-based agents in solving tasks.\n",
            "2. **CHDRL (2020)**: A framework that integrates the advantages of heterogeneous agents to learn policies effectively.\n",
            "3. **Human-Agent Interaction (2023)**: Research on improving the relationship between humans and anthropomorphic agents by focusing on empathy and acceptance of agent mistakes.\n",
            "\n",
            "### Conclusion:\n",
            "LLM-based agents have broken onto the scene recently, driven by advancements in large language models and their integration with external tools and resources. This has opened up new possibilities for AI applications, making these agents more capable and versatile in solving complex tasks.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
