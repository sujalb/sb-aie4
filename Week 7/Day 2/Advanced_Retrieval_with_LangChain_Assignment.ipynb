{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:.env file found at: c:\\src\\mlops\\sb-aie4\\.env\n",
            "INFO:__main__:OpenAI API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = Path.cwd()\n",
        "\n",
        "# Construct the path to the .env file\n",
        "env_path = current_dir.parent.parent / '.env'\n",
        "\n",
        "# Check if the .env file exists\n",
        "if env_path.exists():\n",
        "    logger.info(f\".env file found at: {env_path}\")\n",
        "    # Load environment variables from .env file\n",
        "    load_dotenv(dotenv_path=env_path)\n",
        "else:\n",
        "    logger.error(f\".env file not found at: {env_path}\")\n",
        "\n",
        "# Retrieve the OpenAI API key from environment variables\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Verify that the API key is set\n",
        "if openai.api_key:\n",
        "    logger.info(\"OpenAI API key loaded successfully.\")\n",
        "else:\n",
        "    logger.error(\"Failed to load OpenAI API key.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:cohere_key API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "cohere_key = os.getenv(\"COHERE_API_KEY\")\n",
        "\n",
        "# Verify that the API key is set\n",
        "if cohere_key:\n",
        "    logger.info(\"cohere_key API key loaded successfully.\")\n",
        "else:\n",
        "    logger.error(\"Failed to load cohere_key API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0pDRFEWSXvh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 19628  100 19628    0     0   159k      0 --:--:-- --:--:-- --:--:--  201k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 14747  100 14747    0     0   106k      0 --:--:-- --:--:-- --:--:--  135k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 13888  100 13888    0     0   142k      0 --:--:-- --:--:-- --:--:--  167k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 15109  100 15109    0     0   115k      0 --:--:-- --:--:-- --:--:--  144k\n"
          ]
        }
      ],
      "source": [
        "!curl -o john_wick_1.csv https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
        "!curl -o john_wick_2.csv https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
        "!curl -o john_wick_3.csv https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
        "!curl -o john_wick_4.csv https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    # This line is setting the \"last_accessed_at\" metadata for each document\n",
        "    # It creates a datetime value that is more recent for newer movies (higher i)\n",
        "    # The formula subtracts (4-i) days from the current date and time\n",
        "    # So John Wick 1 (i=1) will be 3 days ago, John Wick 4 (i=4) will be today\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n- Review Title: \"A Masterpiece & Brilliant Sequel\"\\n- URL: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval Chain Response:\n",
            "Yes, there is a review with a rating of 10 for \"John Wick 3\". Here is the URL to that review: /review/rw4854296/?ref_=tt_urv\n",
            "\n",
            "Checking all documents with rating 10:\n",
            "\n",
            "Found a review with rating 10:\n",
            "Movie: John Wick 1\n",
            "Review Title:  The best action revenge film of all time from 2014 so far!\n",
            "\n",
            "Review URL: /review/rw3357633/?ref_=tt_urv\n",
            "Author: ivo-cobra8\n",
            "Review Date: 19 November 2015\n",
            "\n",
            "Found a review with rating 10:\n",
            "Movie: John Wick 1\n",
            "Review Title:  Smoothest action film to come around in a long time\n",
            "\n",
            "Review URL: /review/rw3109271/?ref_=tt_urv\n",
            "Author: IceSkateUpHill\n",
            "Review Date: 22 October 2014\n",
            "\n",
            "Found a review with rating 10:\n",
            "Movie: John Wick 2\n",
            "Review Title:  Entertaining Sequel\n",
            "\n",
            "Review URL: /review/rw7582036/?ref_=tt_urv\n",
            "Author: Johnny_West\n",
            "Review Date: 25 November 2021\n",
            "\n",
            "Found a review with rating 10:\n",
            "Movie: John Wick 3\n",
            "Review Title:  A Masterpiece & Brilliant Sequel\n",
            "\n",
            "Review URL: /review/rw4854296/?ref_=tt_urv\n",
            "Author: ymyuseda\n",
            "Review Date: 15 May 2019\n",
            "\n",
            "Found a review with rating 10:\n",
            "Movie: John Wick 3\n",
            "Review Title:  Makes John Wick a superb trilogy\n",
            "\n",
            "Review URL: /review/rw4860603/?ref_=tt_urv\n",
            "Author: masonsaul\n",
            "Review Date: 17 May 2019\n",
            "\n",
            "Found a review with rating 10:\n",
            "Movie: John Wick 4\n",
            "Review Title:  Not Just The Best John Wick, But Possibly One Of The Best Action Movies Ever\n",
            "\n",
            "Review URL: /review/rw8935367/?ref_=tt_urv\n",
            "Author: cadillac20\n",
            "Review Date: 17 March 2023\n"
          ]
        }
      ],
      "source": [
        "# SB - Added some code for testing\n",
        "response = naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})\n",
        "\n",
        "print(\"Retrieval Chain Response:\")\n",
        "print(response[\"response\"].content)\n",
        "\n",
        "print(\"\\nChecking all documents with rating 10:\")\n",
        "found_10_rating = False\n",
        "for doc in documents:\n",
        "    if doc.metadata[\"Rating\"] == 10:\n",
        "        found_10_rating = True\n",
        "        print(f\"\\nFound a review with rating 10:\")\n",
        "        print(f\"Movie: {doc.metadata['Movie_Title']}\")\n",
        "        print(f\"Review Title: {doc.metadata['Review_Title']}\")\n",
        "        print(f\"Review URL: {doc.metadata['Review_Url']}\")\n",
        "        print(f\"Author: {doc.metadata['Author']}\")\n",
        "        print(f\"Review Date: {doc.metadata['Review_Date']}\")\n",
        "\n",
        "if not found_10_rating:\n",
        "    print(\"No reviews with rating 10 found in the original documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking if the URL from the retrieval chain matches any document:\n",
            "Match found in John Wick 3:\n",
            "Rating: 10\n",
            "Review Title:  A Masterpiece & Brilliant Sequel\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nChecking if the URL from the retrieval chain matches any document:\")\n",
        "retrieval_url = '/review/rw4854296/?ref_=tt_urv'  # URL from the retrieval chain response\n",
        "found_match = False\n",
        "for doc in documents:\n",
        "    if doc.metadata[\"Review_Url\"] == retrieval_url:\n",
        "        found_match = True\n",
        "        print(f\"Match found in {doc.metadata['Movie_Title']}:\")\n",
        "        print(f\"Rating: {doc.metadata['Rating']}\")\n",
        "        print(f\"Review Title: {doc.metadata['Review_Title']}\")\n",
        "        break\n",
        "\n",
        "if not found_match:\n",
        "    print(\"No match found for the URL returned by the retrieval chain.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to seek vengeance against the gangsters who killed his dog and took everything from him. The story is filled with violent action, shootouts, and breathtaking fights as John Wick unleashes a maelstrom of destruction against those who try to chase him.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from rank_bm25) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Overall, opinions on John Wick seem to vary. Some people really enjoyed the action and style of the movie, while others found it to be lacking in plot and substance. It's best to watch the movie and form your own opinion on whether people generally liked it.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I'm sorry, but there are no reviews with a rating of 10 in the provided context.\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'In John Wick, the main character, played by Keanu Reeves, is a retired hitman seeking vengeance for the death of his dog, given to him by his deceased wife. The movie is known for its intense action sequences and fight scenes.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-cohere in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.3.0)\n",
            "Requirement already satisfied: cohere<6.0,>=5.5.6 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-cohere) (5.9.2)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-cohere) (0.3.6)\n",
            "Requirement already satisfied: langchain-experimental>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-cohere) (0.3.2)\n",
            "Requirement already satisfied: pandas>=1.4.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-cohere) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-cohere) (2.8.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (1.35.4)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (1.9.5)\n",
            "Requirement already satisfied: httpx>=0.21.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.27.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.20.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.19.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.32.0.20240712)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (0.1.125)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (8.5.0)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-experimental>=0.3.0->langchain-cohere) (0.3.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas>=1.4.3->langchain-cohere) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas>=1.4.3->langchain-cohere) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas>=1.4.3->langchain-cohere) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas>=1.4.3->langchain-cohere) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere) (1.35.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere) (0.10.2)\n",
            "Requirement already satisfied: anyio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (4.2.0)\n",
            "Requirement already satisfied: certifi in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (1.0.5)\n",
            "Requirement already satisfied: idna in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-cohere) (3.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.3.1)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (2.5.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-cohere) (3.10.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.3->langchain-cohere) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain-cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain-cohere) (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (0.24.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: filelock in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (2024.6.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (4.66.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain<0.4.0,>=0.3.1->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.3.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.0.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (3.0.3)\n",
            "Requirement already satisfied: colorama in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (0.4.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". The URL to that review is: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"In John Wick, after resolving his issues with the Russian mafia, John Wick is visited by mobster Santino D'Antonio who asks him to kill his sister in Rome. When John accomplishes this task, Santino puts a seven-million dollar contract on him, leading to professional killers coming after him. Wick promises to kill Santino.\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What was the overall reception of John Wick among viewers?', 'Were audiences generally satisfied with John Wick?', 'How well was John Wick received by the public in general?']\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that people generally liked John Wick. The action sequences, Keanu Reeves' performance, and the overall entertainment value of the movie were positively highlighted in the reviews.\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Are there any reviews with a perfect rating of 10?', '2. Can you provide me with the links to reviews that received a rating of 10?', \"3. I'm looking for reviews that have been rated 10 - could you share the URLs for those reviews?\"]\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What events unfolded in the movie John Wick?', 'Can you summarize the plot of John Wick?', 'What is the storyline of John Wick?']\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"In John Wick: Chapter 2, the main character, John Wick, is forced back into the world of assassins when an Italian crime lord calls in a favor. Wick is tasked with killing the crime lord's sister in Rome so that the crime lord can take over the criminal organizations. After completing the task, Wick becomes a target with a bounty on his head, leading to professional killers coming after him. Ultimately, Wick seeks to eliminate the crime lord who betrayed him.\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\bhatsu\\AppData\\Local\\Temp\\ipykernel_14608\\3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'John Wick is a movie about a retired hitman seeking vengeance for the death of his beloved dog, given to him by his late wife. The dog was killed by a group of Russian gangsters, leading John Wick back into the world of assassins and violence.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the general consensus on John Wick among viewers?', 'Were audiences in favor of John Wick overall?', 'Was John Wick well-received by the public in general?']\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems like people generally enjoyed John Wick as an action film. The reviews praise the action sequences, Keanu Reeves' performance, and the overall enjoyment of the movie. So, yes, people generally liked John Wick.\""
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Are there any reviews with a perfect rating of 10?', '2. Can you provide me with the links to reviews that received a rating of 10?', '3. I am looking for reviews that have been rated 10. Can you share the URLs of those reviews with me?']\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide a summary of the events in John Wick?', '2. What is the storyline of John Wick?', '3. Can you give me an overview of the plot of John Wick?']\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'In John Wick, the story revolves around an ex-hitman who comes out of retirement to seek revenge on the gangsters who killed his dog and took everything from him. This leads him into a series of violent confrontations and action-packed sequences as he hunts down those responsible for his loss.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that people generally liked John Wick. The reviews talk about how cool, fun, and action-packed the movies are, praising Keanu Reeves' performance and the excellent action sequences.\""
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3.\" Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'In John Wick, the main character seeks revenge on the people who took something he loved from him, which in this case was his dog. This sets off a chain of events involving action, stylish stunts, chaos, and a relatable hero.'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a \"golden dataset\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: ragas\n",
            "Version: 0.1.20\n",
            "Summary: \n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: \n",
            "Location: c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\n",
            "Requires: appdirs, datasets, langchain, langchain-community, langchain-core, langchain-openai, nest-asyncio, numpy, openai, pysbd, tiktoken\n",
            "Required-by: \n",
            "Requirement already satisfied: ragas[datasets] in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.1.20)\n",
            "Requirement already satisfied: numpy in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.26.4)\n",
            "Requirement already satisfied: datasets in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (3.0.0)\n",
            "Requirement already satisfied: tiktoken in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.7.0)\n",
            "Requirement already satisfied: langchain in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.3.1)\n",
            "Collecting langchain-core<0.3 (from ragas[datasets])\n",
            "  Using cached langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: langchain-community in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.3.1)\n",
            "Requirement already satisfied: langchain-openai in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.2.1)\n",
            "Requirement already satisfied: openai>1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.45.0)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.6.0)\n",
            "Requirement already satisfied: appdirs in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.4.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (0.1.125)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (0.5.0)\n",
            "Requirement already satisfied: sniffio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (4.66.5)\n",
            "Requirement already satisfied: filelock in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (2.32.3)\n",
            "Requirement already satisfied: xxhash in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas[datasets]) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (0.24.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain->ragas[datasets]) (2.0.32)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain (from ragas[datasets])\n",
            "  Using cached langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas[datasets])\n",
            "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community->ragas[datasets]) (0.6.7)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community (from ragas[datasets])\n",
            "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai (from ragas[datasets])\n",
            "  Using cached langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tiktoken->ragas[datasets]) (2024.7.24)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas[datasets]) (3.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas[datasets]) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas[datasets]) (0.9.0)\n",
            "Requirement already satisfied: certifi in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas[datasets]) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas[datasets]) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas[datasets]) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3->ragas[datasets]) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3->ragas[datasets]) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3->ragas[datasets]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3->ragas[datasets]) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets->ragas[datasets]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets->ragas[datasets]) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas[datasets]) (3.0.3)\n",
            "Requirement already satisfied: colorama in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tqdm>4->openai>1->ragas[datasets]) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas[datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas[datasets]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas[datasets]) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas[datasets]) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas[datasets]) (1.0.0)\n",
            "Using cached langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
            "Using cached langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
            "Using cached langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
            "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.6\n",
            "    Uninstalling langchain-core-0.3.6:\n",
            "      Successfully uninstalled langchain-core-0.3.6\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.0\n",
            "    Uninstalling langchain-text-splitters-0.3.0:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.0\n",
            "  Attempting uninstall: langchain-openai\n",
            "    Found existing installation: langchain-openai 0.2.1\n",
            "    Uninstalling langchain-openai-0.2.1:\n",
            "      Successfully uninstalled langchain-openai-0.2.1\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.1\n",
            "    Uninstalling langchain-0.3.1:\n",
            "      Successfully uninstalled langchain-0.3.1\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.1\n",
            "    Uninstalling langchain-community-0.3.1:\n",
            "      Successfully uninstalled langchain-community-0.3.1\n",
            "Successfully installed langchain-0.2.16 langchain-community-0.2.17 langchain-core-0.2.41 langchain-openai-0.1.25 langchain-text-splitters-0.2.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: ragas 0.1.20 does not provide the extra 'datasets'\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-cohere 0.3.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.2.17 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-huggingface 0.1.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip show ragas\n",
        "!pip install --upgrade \"ragas[datasets]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ragas[datasets] in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.1.20)\n",
            "Requirement already satisfied: numpy in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.26.4)\n",
            "Requirement already satisfied: datasets in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (3.0.0)\n",
            "Requirement already satisfied: tiktoken in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.7.0)\n",
            "Requirement already satisfied: langchain in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.2.16)\n",
            "Requirement already satisfied: langchain-core<0.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.2.41)\n",
            "Requirement already satisfied: langchain-community in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.2.17)\n",
            "Requirement already satisfied: langchain-openai in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.1.25)\n",
            "Requirement already satisfied: openai>1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.45.0)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.6.0)\n",
            "Requirement already satisfied: appdirs in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas[datasets]) (1.4.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (0.1.125)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas[datasets]) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (0.5.0)\n",
            "Requirement already satisfied: sniffio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas[datasets]) (4.66.5)\n",
            "Requirement already satisfied: filelock in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (2.32.3)\n",
            "Requirement already satisfied: xxhash in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas[datasets]) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas[datasets]) (0.24.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain->ragas[datasets]) (2.0.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain->ragas[datasets]) (0.2.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community->ragas[datasets]) (0.6.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tiktoken->ragas[datasets]) (2024.7.24)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas[datasets]) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas[datasets]) (3.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas[datasets]) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas[datasets]) (0.9.0)\n",
            "Requirement already satisfied: certifi in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas[datasets]) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas[datasets]) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas[datasets]) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3->ragas[datasets]) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3->ragas[datasets]) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3->ragas[datasets]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3->ragas[datasets]) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets->ragas[datasets]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets->ragas[datasets]) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas[datasets]) (3.0.3)\n",
            "Requirement already satisfied: colorama in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tqdm>4->openai>1->ragas[datasets]) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas[datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas[datasets]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas[datasets]) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas[datasets]) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas[datasets]) (1.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: ragas 0.1.20 does not provide the extra 'datasets'\n"
          ]
        }
      ],
      "source": [
        "!pip install \"ragas[datasets]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: ragas\n",
            "Version: 0.1.20\n",
            "Summary: \n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: \n",
            "Location: c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\n",
            "Requires: appdirs, datasets, langchain, langchain-community, langchain-core, langchain-openai, nest-asyncio, numpy, openai, pysbd, tiktoken\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain\n",
            "Version: 0.3.2\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\n",
            "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community, ragas\n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.3.1\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\n",
            "Requires: aiohttp, dataclasses-json, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-experimental, ragas\n",
            "---\n",
            "Name: google-cloud-aiplatform\n",
            "Version: 1.69.0\n",
            "Summary: Vertex AI API client library\n",
            "Home-page: https://github.com/googleapis/python-aiplatform\n",
            "Author: Google LLC\n",
            "Author-email: googleapis-packages@google.com\n",
            "License: Apache 2.0\n",
            "Location: c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\n",
            "Requires: docstring-parser, google-api-core, google-auth, google-cloud-bigquery, google-cloud-resource-manager, google-cloud-storage, packaging, proto-plus, protobuf, pydantic, shapely\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show ragas langchain langchain-community google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
            "c:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
            "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        },
        {
          "ename": "PydanticUserError",
          "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.8/u/custom-json-schema",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ragas\u001b[38;5;241m.\u001b[39m__version__)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\adaptation.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_factory\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricWithLLM\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\llms\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     BaseRagasLLM,\n\u001b[0;32m      3\u001b[0m     LangchainLLMWrapper,\n\u001b[0;32m      4\u001b[0m     LlamaIndexLLMWrapper,\n\u001b[0;32m      5\u001b[0m     llm_factory,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangchainLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaIndexLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m ]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\llms\\base.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generation, LLMResult\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOpenAI\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_basemodel_subclass\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[0;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     46\u001b[0m _BM \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BM\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mBaseModel)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:379\u001b[0m\n\u001b[0;32m    375\u001b[0m     parsed: Optional[_DictOrPydantic]\n\u001b[0;32m    376\u001b[0m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mBaseChatOpenAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:205\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[0;32m    203\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[1;32m--> 205\u001b[0m \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypes_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:534\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[1;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_pydantic_core_schema__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\main.py:643\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[1;34m(cls, source, handler)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:512\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    509\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:784\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type_stack\u001b[38;5;241m.\u001b[39mpush(obj):\n\u001b[1;32m--> 784\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:591\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    579\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    581\u001b[0m         inner_schema,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    588\u001b[0m     )\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[1;32m--> 591\u001b[0m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    592\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    593\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[0;32m    594\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    595\u001b[0m         ],\n\u001b[0;32m    596\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[0;32m    597\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    598\u001b[0m     )\n\u001b[0;32m    599\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    600\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:947\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    942\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    943\u001b[0m     field_info: FieldInfo,\n\u001b[0;32m    944\u001b[0m     decorators: DecoratorInfos,\n\u001b[0;32m    945\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[0;32m    946\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[0;32m    949\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    950\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    955\u001b[0m     )\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1134\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(source_type, annotations, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator)\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1890\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[1;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[0;32m   1885\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1886\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[0;32m   1887\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[0;32m   1888\u001b[0m     )\n\u001b[1;32m-> 1890\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[0;32m   1892\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1871\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1869\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, source_type)\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1873\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:789\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[1;32m--> 789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:871\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    869\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_generic_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arbitrary_types:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arbitrary_type_schema(obj)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:895\u001b[0m, in \u001b[0;36mGenerateSchema._match_generic_type\u001b[1;34m(self, obj, origin)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_property\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39morigin_is_union(origin):\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_union_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuple_schema(obj)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1207\u001b[0m, in \u001b[0;36mGenerateSchema._union_schema\u001b[1;34m(self, union_type)\u001b[0m\n\u001b[0;32m   1205\u001b[0m         nullable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1207\u001b[0m         choices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1210\u001b[0m     s \u001b[38;5;241m=\u001b[39m choices[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:514\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    512\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[1;32m--> 514\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_get_pydantic_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     metadata_schema \u001b[38;5;241m=\u001b[39m resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefs\u001b[38;5;241m.\u001b[39mdefinitions)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2227\u001b[0m, in \u001b[0;36m_extract_get_pydantic_json_schema\u001b[1;34m(tp, schema)\u001b[0m\n\u001b[0;32m   2225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[0;32m   2226\u001b[0m         cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m   2228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2229\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2230\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom-json-schema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2231\u001b[0m         )\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;66;03m# handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tp, \u001b[38;5;28mtype\u001b[39m(Annotated[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n",
            "\u001b[1;31mPydanticUserError\u001b[0m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.8/u/custom-json-schema"
          ]
        }
      ],
      "source": [
        "import ragas\n",
        "print(ragas.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [
        {
          "ename": "PydanticUserError",
          "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.8/u/custom-json-schema",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_qa_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate a synthetic dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m create_qa_dataset(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Wick movie reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,  \u001b[38;5;66;03m# Adjust the size as needed\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m  \u001b[38;5;66;03m# Adjust based on your document lengths\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\adaptation.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_factory\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricWithLLM\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\llms\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     BaseRagasLLM,\n\u001b[0;32m      3\u001b[0m     LangchainLLMWrapper,\n\u001b[0;32m      4\u001b[0m     LlamaIndexLLMWrapper,\n\u001b[0;32m      5\u001b[0m     llm_factory,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangchainLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaIndexLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m ]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\llms\\base.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generation, LLMResult\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOpenAI\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_basemodel_subclass\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[0;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     46\u001b[0m _BM \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BM\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mBaseModel)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:379\u001b[0m\n\u001b[0;32m    375\u001b[0m     parsed: Optional[_DictOrPydantic]\n\u001b[0;32m    376\u001b[0m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mBaseChatOpenAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:205\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[0;32m    203\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[1;32m--> 205\u001b[0m \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypes_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:534\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[1;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_pydantic_core_schema__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\main.py:643\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[1;34m(cls, source, handler)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:512\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    509\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:784\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type_stack\u001b[38;5;241m.\u001b[39mpush(obj):\n\u001b[1;32m--> 784\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:591\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    579\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    581\u001b[0m         inner_schema,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    588\u001b[0m     )\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[1;32m--> 591\u001b[0m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    592\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    593\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[0;32m    594\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    595\u001b[0m         ],\n\u001b[0;32m    596\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[0;32m    597\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    598\u001b[0m     )\n\u001b[0;32m    599\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    600\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:947\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    942\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    943\u001b[0m     field_info: FieldInfo,\n\u001b[0;32m    944\u001b[0m     decorators: DecoratorInfos,\n\u001b[0;32m    945\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[0;32m    946\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[0;32m    949\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    950\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    955\u001b[0m     )\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1134\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(source_type, annotations, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator)\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1890\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[1;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[0;32m   1885\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1886\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[0;32m   1887\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[0;32m   1888\u001b[0m     )\n\u001b[1;32m-> 1890\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[0;32m   1892\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1871\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1869\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, source_type)\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1873\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:789\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[1;32m--> 789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:871\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    869\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_generic_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arbitrary_types:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arbitrary_type_schema(obj)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:895\u001b[0m, in \u001b[0;36mGenerateSchema._match_generic_type\u001b[1;34m(self, obj, origin)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_property\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39morigin_is_union(origin):\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_union_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuple_schema(obj)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1207\u001b[0m, in \u001b[0;36mGenerateSchema._union_schema\u001b[1;34m(self, union_type)\u001b[0m\n\u001b[0;32m   1205\u001b[0m         nullable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1207\u001b[0m         choices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1210\u001b[0m     s \u001b[38;5;241m=\u001b[39m choices[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:514\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    512\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[1;32m--> 514\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_get_pydantic_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     metadata_schema \u001b[38;5;241m=\u001b[39m resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefs\u001b[38;5;241m.\u001b[39mdefinitions)\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2227\u001b[0m, in \u001b[0;36m_extract_get_pydantic_json_schema\u001b[1;34m(tp, schema)\u001b[0m\n\u001b[0;32m   2225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[0;32m   2226\u001b[0m         cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m   2228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2229\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2230\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom-json-schema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2231\u001b[0m         )\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;66;03m# handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tp, \u001b[38;5;28mtype\u001b[39m(Annotated[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n",
            "\u001b[1;31mPydanticUserError\u001b[0m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.8/u/custom-json-schema"
          ]
        }
      ],
      "source": [
        "from ragas.datasets import create_qa_dataset\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "synthetic_data = create_qa_dataset(\n",
        "    \"John Wick movie reviews\",\n",
        "    size=50,  # Adjust the size as needed\n",
        "    context_length=512  # Adjust based on your document lengths\n",
        ")\n",
        "\n",
        "# The dataset will contain questions, contexts, and answers\n",
        "questions = synthetic_data['question']\n",
        "contexts = synthetic_data['context']\n",
        "answers = synthetic_data['answer']\n",
        "\n",
        "print(f\"Generated {len(questions)} synthetic QA pairs.\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Question: {questions[0]}\")\n",
        "print(f\"Context: {contexts[0]}\")\n",
        "print(f\"Answer: {answers[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ragas in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.1.20)\n",
            "Requirement already satisfied: numpy in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.26.4)\n",
            "Requirement already satisfied: datasets in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (3.0.0)\n",
            "Requirement already satisfied: tiktoken in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.7.0)\n",
            "Requirement already satisfied: langchain in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.2.16)\n",
            "Requirement already satisfied: langchain-core<0.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.2.41)\n",
            "Requirement already satisfied: langchain-community in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.2.17)\n",
            "Requirement already satisfied: langchain-openai in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.1.25)\n",
            "Requirement already satisfied: openai>1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.45.0)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (0.1.125)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (0.5.0)\n",
            "Requirement already satisfied: sniffio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (4.66.5)\n",
            "Requirement already satisfied: filelock in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (2.32.3)\n",
            "Requirement already satisfied: xxhash in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (0.24.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain->ragas) (2.0.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain->ragas) (0.2.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tiktoken->ragas) (2024.7.24)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3->ragas) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3->ragas) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3->ragas) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
            "Requirement already satisfied: colorama in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Evaluate each retriever with Ragas metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ragas in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.1.20)\n",
            "Requirement already satisfied: langchain in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.2.16)\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-community in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (0.2.17)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.69.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: numpy in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.26.4)\n",
            "Requirement already satisfied: datasets in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (3.0.0)\n",
            "Requirement already satisfied: tiktoken in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.7.0)\n",
            "Requirement already satisfied: langchain-core<0.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.2.41)\n",
            "Requirement already satisfied: langchain-openai in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.1.25)\n",
            "Requirement already satisfied: openai>1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.45.0)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (3.10.5)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (0.1.125)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
            "  Downloading google_api_core-2.20.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-auth<3.0.0dev,>=2.14.1 (from google-cloud-aiplatform)\n",
            "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform)\n",
            "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from google-cloud-aiplatform) (5.27.3)\n",
            "Requirement already satisfied: packaging>=14.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from google-cloud-aiplatform) (24.1)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting shapely<3.0.0dev (from google-cloud-aiplatform)\n",
            "  Downloading shapely-2.0.6-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
            "Collecting docstring-parser<1 (from google-cloud-aiplatform)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
            "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
            "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
            "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
            "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
            "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
            "  Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform)\n",
            "  Downloading google_crc32c-1.6.0-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langchain-core<0.3->ragas) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (0.5.0)\n",
            "Requirement already satisfied: sniffio in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from openai>1->ragas) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: filelock in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: xxhash in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from datasets->ragas) (0.24.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tiktoken->ragas) (2024.7.24)\n",
            "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
            "  Downloading grpcio-1.66.2-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3->ragas) (3.0.0)\n",
            "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\src\\mlops\\sb-aie4\\.conda\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Downloading google_cloud_aiplatform-1.69.0-py2.py3-none-any.whl (5.3 MB)\n",
            "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 1.3/5.3 MB 6.7 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 3.1/5.3 MB 8.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 4.7/5.3 MB 8.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 5.3/5.3 MB 7.8 MB/s eta 0:00:00\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading google_api_core-2.20.0-py3-none-any.whl (142 kB)\n",
            "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
            "Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n",
            "Downloading google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl (341 kB)\n",
            "Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
            "Downloading shapely-2.0.6-cp312-cp312-win_amd64.whl (1.4 MB)\n",
            "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
            "   ----------------------------- ---------- 1.0/1.4 MB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.4/1.4 MB 5.8 MB/s eta 0:00:00\n",
            "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
            "Downloading google_crc32c-1.6.0-cp312-cp312-win_amd64.whl (33 kB)\n",
            "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
            "Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
            "Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
            "Downloading grpcio_status-1.66.2-py3-none-any.whl (14 kB)\n",
            "Downloading grpcio-1.66.2-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 1.0/4.3 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 2.6/4.3 MB 6.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 3.9/4.3 MB 6.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.3/4.3 MB 6.8 MB/s eta 0:00:00\n",
            "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: shapely, pyasn1, proto-plus, grpcio, googleapis-common-protos, google-crc32c, docstring-parser, cachetools, rsa, pyasn1-modules, grpcio-status, google-resumable-media, grpc-google-iam-v1, google-auth, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.66.0\n",
            "    Uninstalling grpcio-1.66.0:\n",
            "      Successfully uninstalled grpcio-1.66.0\n",
            "Successfully installed cachetools-5.5.0 docstring-parser-0.16 google-api-core-2.20.0 google-auth-2.35.0 google-cloud-aiplatform-1.69.0 google-cloud-bigquery-3.26.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.5 google-cloud-storage-2.18.2 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.65.0 grpc-google-iam-v1-0.13.1 grpcio-1.66.2 grpcio-status-1.66.2 proto-plus-1.24.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9 shapely-2.0.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\~rpc'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade ragas langchain langchain-community google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[89], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     context_relevancy, context_recall, faithfulness, answer_relevancy\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      6\u001b[0m retrievers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25\u001b[39m\u001b[38;5;124m\"\u001b[39m: bm25_retriever,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive Vector\u001b[39m\u001b[38;5;124m\"\u001b[39m: naive_retriever,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble\u001b[39m\u001b[38;5;124m\"\u001b[39m: ensemble_retriever\n\u001b[0;32m     13\u001b[0m }\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\adaptation.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_factory\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricWithLLM\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\llms\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     BaseRagasLLM,\n\u001b[0;32m      3\u001b[0m     LangchainLLMWrapper,\n\u001b[0;32m      4\u001b[0m     LlamaIndexLLMWrapper,\n\u001b[0;32m      5\u001b[0m     llm_factory,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangchainLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaIndexLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m ]\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\ragas\\llms\\base.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VertexAI\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_community\\chat_models\\vertexai.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGeneration, ChatGenerationChunk, ChatResult\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pre_init\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     _VertexAICommon,\n\u001b[0;32m     34\u001b[0m     is_codey_model,\n\u001b[0;32m     35\u001b[0m     is_gemini_model,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     38\u001b[0m     load_image_from_gcs,\n\u001b[0;32m     39\u001b[0m     raise_vertex_import_error,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[1;32mc:\\src\\mlops\\sb-aie4\\.conda\\Lib\\site-packages\\langchain_community\\llms\\vertexai.py:209\u001b[0m\n\u001b[0;32m    200\u001b[0m             params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m params\n\u001b[0;32m    204\u001b[0m \u001b[38;5;129;43m@deprecated\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43msince\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremoval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43malternative_import\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlangchain_google_vertexai.VertexAI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mVertexAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m_VertexAICommon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Google Vertex AI large language models.\"\"\"\u001b[39;49;00m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-bison\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
            "\u001b[1;31mTypeError\u001b[0m: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases"
          ]
        }
      ],
      "source": [
        "from ragas.metrics import (\n",
        "    context_relevancy, context_recall, faithfulness, answer_relevancy\n",
        ")\n",
        "from ragas import evaluate\n",
        "\n",
        "retrievers = {\n",
        "    \"BM25\": bm25_retriever,\n",
        "    \"Naive Vector\": naive_retriever,\n",
        "    \"Parent Document\": parent_document_retriever,\n",
        "    \"Compression\": compression_retriever,\n",
        "    \"Multi-Query\": multi_query_retriever,\n",
        "    \"Ensemble\": ensemble_retriever\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, retriever in retrievers.items():\n",
        "    eval_result = evaluate(\n",
        "        retriever=retriever,\n",
        "        questions=questions,\n",
        "        contexts=contexts,\n",
        "        ground_truths=answers,\n",
        "        metrics=[\n",
        "            context_relevancy,\n",
        "            context_recall,\n",
        "            faithfulness,\n",
        "            answer_relevancy\n",
        "        ]\n",
        "    )\n",
        "    results[name] = eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Compile and analyze results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[90], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for easy comparison\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mresults\u001b[49m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Add columns for cost and latency (you'll need to measure these)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]  \u001b[38;5;66;03m# Example values, replace with actual\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for easy comparison\n",
        "df_results = pd.DataFrame(results).T\n",
        "\n",
        "# Add columns for cost and latency (you'll need to measure these)\n",
        "df_results['Cost'] = [1, 2, 3, 4, 5, 6]  # Example values, replace with actual\n",
        "df_results['Latency'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]  # Example values, replace with actual\n",
        "\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's write a small paragraph analyzing the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_retriever = df_results['context_relevancy'].idxmax()\n",
        "print(f\"Based on our analysis, the {best_retriever} retriever performs best for this particular dataset.\")\n",
        "print(\"Here's why:\")\n",
        "print(\"1. Performance: It achieves the highest context relevancy score, indicating it retrieves the most relevant information.\")\n",
        "print(f\"2. Cost: While not the cheapest option, its cost of {df_results.loc[best_retriever, 'Cost']} is justified by its superior performance.\")\n",
        "print(f\"3. Latency: With a latency of {df_results.loc[best_retriever, 'Latency']} seconds, it provides a good balance between speed and accuracy.\")\n",
        "print(\"4. Dataset Characteristics: Given that our dataset consists of movie reviews, this retriever's ability to capture semantic meaning and context aligns well with the nuanced language often found in reviews.\")\n",
        "print(\"5. Trade-offs: While other retrievers may excel in specific areas (e.g., BM25 for keyword matching), this retriever provides the best overall performance across our evaluation metrics.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
